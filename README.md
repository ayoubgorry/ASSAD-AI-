# üß† Architecture G√©n√©rale ‚Äì ASSAD AI (CAN 2025)

## 1. Description g√©n√©rale

**ASSAD AI ‚Äì CAN 2025 Assistant** est un syst√®me intelligent de question-r√©ponse bas√© sur l‚Äôapproche  
**RAG (Retrieval-Augmented Generation)**, con√ßu pour fournir des r√©ponses **factuelles, coh√©rentes et contextualis√©es** sur la Coupe d‚ÄôAfrique des Nations 2025 organis√©e au Maroc.

Le syst√®me combine :
- une **interface utilisateur web moderne**
- une **API backend performante**
- une **base de donn√©es vectorielle**
- un **LLM de derni√®re g√©n√©ration**

L‚Äôobjectif principal est d‚Äô√©viter les hallucinations du mod√®le en s‚Äôappuyant exclusivement sur des **donn√©es officielles structur√©es** (JSON) li√©es √† la CAN 2025.

---

## 2. Vue d‚Äôensemble de l‚Äôarchitecture

Le syst√®me est organis√© selon une architecture **client‚Äìserveur**, avec un pipeline RAG au c≈ìur du traitement.

![Architecture ASSAD AI ‚Äì CAN 2025](./assets/arch.png)

---
La description d√©taill√©e de l‚Äôarchitecture du syst√®me est disponible ici :  
‚û°Ô∏è [Voir l‚Äôarchitecture g√©n√©rale](./ARCHITECTURE.md)

## 3. Composants principaux

### 3.1 Frontend (Interface utilisateur)

**Technologies**
- React.js
- Tailwind CSS

**R√¥le**
- Fournir une interface de chat interactive
- Envoyer les questions utilisateur au backend via une requ√™te HTTP
- Afficher les r√©ponses g√©n√©r√©es par l‚ÄôIA

**Fonctionnement**
- L‚Äôutilisateur saisit une question
- Une requ√™te `POST /chat` est envoy√©e au backend
- La r√©ponse JSON est affich√©e dans l‚Äôinterface de chat

---

### 3.2 Backend (API & orchestration)

**Technologies**
- Python
- FastAPI
- LangChain

**R√¥le**
- Recevoir les requ√™tes utilisateur
- Orchestrer la cha√Æne RAG
- Retourner une r√©ponse finale format√©e

**Composants**
- API FastAPI
- Cha√Æne RAG (Retriever, Prompt Builder, LLM Generator)

---

## 4. Pipeline RAG (Retrieval-Augmented Generation)

Le pipeline RAG est le c≈ìur du syst√®me.

### 4.1 Retriever
- Convertit la question utilisateur en embedding
- Effectue une recherche par similarit√© dans FAISS
- R√©cup√®re les **k = 20 documents** les plus pertinents

### 4.2 Prompt Builder
- Assemble les documents r√©cup√©r√©s
- Construit un prompt enrichi (question + contexte)
- Garantit que le LLM reste ancr√© dans les donn√©es r√©elles

### 4.3 LLM Generator
- Envoie le prompt final au mod√®le de langage
- G√©n√®re une r√©ponse en fran√ßais
- Fonctionne en mode d√©terministe (temperature = 0)

---

## 5. Base de donn√©es vectorielle

### 5.1 FAISS

**R√¥le**
- Stocker les embeddings des documents
- Permettre une recherche rapide par similarit√© s√©mantique

**Caract√©ristiques**
- Recherche vectorielle approximative
- Embeddings de dimension **384**
- Tr√®s faible latence

---

### 5.2 Embeddings

**Mod√®le**
- Sentence Transformers ‚Äì `all-MiniLM-L6-v2`

**R√¥le**
- Transformer le texte en vecteurs num√©riques
- Capturer la s√©mantique des questions et documents

---

## 6. Sources de donn√©es

Les donn√©es sont stock√©es sous forme de fichiers **JSON** :

- Matchs
- √âquipes
- Joueurs
- Stades

### Indexation offline
- Les donn√©es sont trait√©es et vectoris√©es **avant l‚Äôex√©cution**
- L‚Äôindex FAISS est g√©n√©r√© une seule fois
- Aucun recalcul d‚Äôembeddings en production

---

## 7. Mod√®le de langage (LLM)

**Mod√®le utilis√©**
- Google Gemini 3 Pro

**R√¥le**
- G√©n√©ration de r√©ponses en langage naturel
- Exploitation du contexte fourni par le pipeline RAG

**Configuration**
- Temperature = 0 (r√©ponses stables et factuelles)

---

## 8. Flux de donn√©es global

1. L‚Äôutilisateur pose une question via le frontend
2. Le frontend envoie une requ√™te `POST /chat`
3. Le backend d√©clenche la cha√Æne RAG
4. Le retriever interroge FAISS
5. Les documents pertinents sont inject√©s dans le prompt
6. Le LLM g√©n√®re une r√©ponse
7. La r√©ponse est renvoy√©e au frontend

---

## 9. Avantages de cette architecture

- R√©duction des hallucinations
- R√©ponses bas√©es sur des donn√©es r√©elles
- Architecture modulaire et scalable
- S√©paration claire des responsabilit√©s
- Facilit√© de maintenance et d‚Äô√©volution

---

## 10. Conclusion

Cette architecture garantit un syst√®me IA **robuste, fiable et explicable**, parfaitement adapt√© √† un contexte critique comme celui de la **CAN 2025**, o√π l‚Äôexactitude des informations est essentielle.

Elle constitue une base solide pour une application acad√©mique, professionnelle ou industrielle int√©grant les technologies modernes de l‚ÄôIA g√©n√©rative.
